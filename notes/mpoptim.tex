% notes on coupling coefficients for SU(2)
% $Id$
\documentclass{article}[10pt]
\usepackage{fullpage}
\usepackage{amsmath}

\input{macros}

   \newcommand{\qninejc}[9]{\mbox{$\left( \begin{array}{ccc} \!{#1}\! &
                                \!{#2}\! & \!{#3}\! \\ \!{#4}\! & \!{#5}\! &
                                \!{#6}\! \\ \!{#7}\! & \!{#8}\! & \!{#9}\! 
                                \end{array} \right)$}}

\newcommand{\ad}{\ddagger}
\newcommand{\invad}{{\ddagger \ddagger \ddagger}}

\begin{document}

\title{Optimizing operations on matrix product states}

\author{I. P. McCulloch}
\date{\today}

\maketitle

\section{F-matrix update}

let $\dim(a') = \dim(a) = M$, be the dimension of the MPO. Let $\dim(s') = \dim(s) = d$, the dimension
of the local basis. Let $\dim{i} = \dim{j} = \dim{i'} = \dim{j'} = D$ be the number of symmetry sectors
in the basis. Thus the total demension of the MPO is $dM \times dM$.

The contraction from the right-hand side to build an F-matrix is,

\begin{equation}
F'^{a'} = A^{s'} F^a B^{\dagger,s} M^{s's}_{a'a}
\end{equation}

(Note: nonabelianmp.tex might have the complex conjugation in the wrong place? In any event,
what the code actually implements is as written above) In detail, 
\beq
\itensor{F'}{a'}_{i'j'} =
\sum_{a,i,j,k,s,s'}
\qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'}
%{j}{a}{i}{s}{k}{s'}{j'}{a'}{i'}
\itensor{M}{k}^{s's}_{a'a} \itensor{A}{s'}_{i'i} \itensor{B^{*}}{s}_{j'j} 
\itensor{F}{a}_{ij}
\eeq

This is implemented by the function 
\begin{verbatim}Fprime = operator_prod(M, A, F, herm(B))\end{verbatim}

The question is, what is the most efficient way to evaluate this product? The naive way is to sum over
$k$, $s'$, $s$, $a'$, $a$, so a loop of up to $d^2 M^2$ operations, each involves roughly $D^2$ multiplies
of $(m/D)^3$, for a total of $d^2 M^2 m^3 / D$ operations. This is a bound - more precisely, the
$d^2 M^2$ term is the number of non-zero matrix elements in the MPO, which is typically $O(dM)$, eg with 
a prefactor of perhaps 2. But in principle, even if the MPO isn't sparse, the cost of the operation is bounded
by $d M m^3 / D$, because we can construct the intermediate products,
\beq
G^{s,a}_{j'ij} = F^a_{ij} B^{*s}_{j'j}
\eeq
and there will be roughly $D^2 dM$ such terms (depends on the combinations allowed by the fusion rules), 
each of which has a cost of
$(m/D)^3$, so the total cost is roughly $dM m^3/D$.
It is tempting to sum over $j$ here, but we cannot because the coupling coefficient appears and
this depends on the other quantum numbers. If the coupling coefficients factorize (eg for abelian quantum numbers)
then we could sum over $j$. Note that not all combinations of $(s,a)$ are likely to be needed for a given MPO.

Given these intermediate terms, we can form the sums
\beq
H^{(s'a'i')}_{ij'} = \sum_{s a k j}
\qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'} \:
M^{[k]s's}_{a'a}
G^{s,a}_{j'ij}
\eeq
and there will again be roughly $D^2 dM$ of these.
Note that the only appearance of $s'$, $a'$, $i'$ here is in the coupling coefficient.
It may be possible to optimize the formation of these sums by finding common factors.

From the $H$ matrices, we can obtain the final result,
\beq
F^{a'}_{i'j'} = \sum_{i s'} A^{s'}_{i'i} H^{(s'a'i')}_{ij'}
\eeq
which again has $D^2 dM$ terms, for a total cost of $d M m^3 / D$.

We can assume that $F$ and $B$ are dense -- that is, they contain all combinations matrix elements that are allowed
by symmetry. We could assume that $H$ is dense too. Note however that all non-zero elements must correspond
to a non-zero $9j$ coefficient.

If two $H$ matrices with the same $s',i',i$ but different $a',j'$ turn out to be the same matrix (up to
some constant) then we will end up repeating the same matrix multiplication in the final step.
For this to occur, they need to be the same $G$ matrix, which is a function of $j'$, so in practice
this can only happen if $a'$ is different.

In addition, if the MPO is triangular, then $F^{[M]}$ (the last element) will be the identity. Best to treat this
as a special case, since it is difficult to detect this using the $G$, $H$ formulation.


\section{E-matrix update}

We can decompose the E-matrix update similarly. The contraction is
\beq
\itensor{E'}{a}_{ij} = \sum_{a',i',j',k,s',s}
\frac{2i'+1}{2i+1}
\qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'}
\itensor{E}{a'}_{i'j'}
\itensor{M}{k}^{*s's}_{a'a} \itensor{A}{s'}^*_{i'i} \itensor{B}{s}_{j'j}
\eeq
Again the conjugation differs slightly from nonabelianmp.tex, but the above is what is actually implemented,
and this makes sense because the contraction from the left is analagous to a bra vector, and we finally
want to perform an operation like $E^\dagger F$, so the conjugation of $M$ cancels out in the final operation.

This is implemented in the toolkit as
\begin{verbatim}Eprime = operator_prod(herm(M), herm(A), E, B)\end{verbatim}

We again define a temporary matrix $J$,
\beq
J^{s'a'}_{i'ij'} = A^{*s'}_{i'i} E^{a'}_{i'j'}
\eeq
from which we form $K$-matrices,
\beq
K^{sa}_{j'ij} = \sum_{s'a'ki'} \frac{2i'+1}{2i+1} \qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'}
M^{[k]s's}_{a'a} J^{s'a'}_{i'ij'}
\eeq
from which we obtain the final result,
\beq
E'^{a}_{ij} = \sum_{j's} K^{sa}_{j'ij} B^{s}_{j'j}
\eeq

\subsection{orthogonality optimizations}

If The A-matrices are right-orthogonal then we have an identity (ignoring, for the moment, coupling coefficients),
\beq
\sum_{s,j} A^{s}_{ij} B^{\dagger s}_{jk} = \delta_{ik} \; I
\label{eq:ConstantPart}
\eeq
where $I$ here represents the identity matrix in the appropriate subspace.
Hence, if we have an existing component $F^{a}_{ij}$ is proportional to the identity,
$F^{a}_{ij} = x^a_i I$, then if $M$ has an appropriate form, then $F'$ might also have some component that is
proportional to the identity. The main example of this is where a component of the MPO is the identity itself.
This is easily implemented for triangular MPO's, where we know that the identity component is $E^0$ and $F^{M-1}$
respectively and this is easy to take advantage of. But in principle, there is much more generality available,
eg if a component of an $E$ or $F$ matrix commutes with the symmetries of the MPS, such as the particle number
operator or even the spin vector operator.

In order for this to occur, we require $i=j$, so let
$F^{a}_{ij} = \delta_{ij} x_i I$, for some subset of $\{a,i,j\}$. Now in order for this to propogate through
the summations, we require that a term in \refeq{eq:ConstantPart} appears.  That is,
for some fixed values of $i'$,$j'$,$a'$ we have,
\beq
\sum_{i=j,k,a,s',s}
\qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'} \:
M^{[k]s's}_{a'a} 
F^{a}_{ij}
A^{s'}_{i'i} 
B^{*s}_{j'j} 
= \mbox{const}
\eeq
And since we have the orthogonality (need to check the coupling coefficient here),
\beq
\sum_{i=j,k,a}
\qninejsq{j}{s}{j'}{a}{k}{a'}{i}{s'}{i'} \:
M^{[k]s's}_{a'a} x^a_i
= c^{a'}_{i'j'} \: \delta_{s's}
\eeq
where at the end we have some constant (that depends on $a'$, $i'$, $j'$).

So, in summary, in order to get a constant value at the end for some component $F'^{a'}_{i'j'}$, we require
\begin{itemize}
\item All components $F^a$ are proportional to the identity, for every non-zero element in $M_{a'a}$.
\item For each non-zero elements in $M_{a'a}$, the local operators are diagonal (we don't however require that $k$ is scalar).
$M_{a'a}^{s's} = p^{a'a}_{s} \delta_{s's} \: I$
\item The summation over $k$, $a$, $i$, $j$ gives a result that is independent of $s'$,$s$. Note that the dependence
on $s'$,$s$ is only through the coupling coefficients.
\end{itemize}

This is rather a complicated procedure, and of limited usefulness. Although a key exception would be string
operators that commute with symmetries, eg $(-1)^N$. These occur in transfer operators when evaluating string
correlations with mp-isymmetry and in triangular MPO's with unitary operators on the diagonal. But for an iMPS,
even though the eigenvectors of the transfer matrix might commute with the symmetry group, this is only at the
fixed point. To recognise that the symmery is possible, we would need to determine this in advance, ie if we
can determine that the MPO commutes with the global symmetries, then we can simply impose that the fixed point
$E$ and $F$ matrices have a particular structure. 

An example of this is the triangular MPO for the number of particles per unit cell. The $2 \times 2$ MPO is purely
a function of the quantum numbers, and is a prime candidate for these optimizations.  

\subsection{Special case for the identity operator}

As a simpler procedure, we consider detecting the identity operator. If the MPO is upper-triangular with Identity at
the extremes, then we know that $F^{M-1} = I$, but there are cases such as finite MPO's, and degenerate cases of
product MPO's, where components may be identity (or proportional to the identity) 

\end{document}
