\documentclass{article}
\input{macros}
\usepackage{bbold} % for \mathbb, \textbb font

\newcommand{\norm}[1]{\Vert\,#1\,\Vert}

\begin{document}

\title{Magnus integrators}

\author{I. P. McCulloch}
\date{\today}

\maketitle

\section{Krylov Spaces}

Ket $\ket{k_0}$ be the starting vector of our Krylov subspace,
and we perform $n$ matrix multiplies, to give the Krylov subspace
$\mathrm{Sp}\{H^i \ket{k_0}; i=0\ldots n\}$. We wish to evaluate
\beq
\ket{\psi} = \sum_{i = 0}^n \; c_i \; \ket{k_i} \; ,
\eeq
where the $\ket{k_i}$ are orthonormal.
We do this by constructing directly the density matrix of $\ket{\psi}$.
These are, for example the left reduced density matrix,
\beq
\rho^L = \sum_{i,j=0}^n c^{}_i c^\dagger_j \; E_{0i} \, k_i \, E_{ij} \, k_j^\dagger \, E_{j0}
\eeq
where $E_{ij}$ is the operator that maps between the matrix spaces
of $i$ and $j$, $E_{ij} = E_{ji}^\dagger$, $E_{ii} = \mathbb{1}$.

Now, we measure the fidelity of $\ket{\psi}$, via the variance
\beq
r^2 = \norm{\ket{\psi^0} - \ket{\psi}}^2 
= 2(1 - \Re \braket{\psi^0}{\psi}) \; ,
\eeq
where $\ket{\psi^0}$ is our numerical approximation to $\ket{\psi}$.
Expanding this, and assuming the coefficients $c_i$ are known exactly,
\beq
r^2 = 2 \left( 1 - \sum_{i=0}^n |c_i|^2 \Re \braket{k_i^0}{k_i} \right) \; .
\eeq
For equal contributions to the error from each term in the Krylov expansion, we
require 
\beq
\norm{\ket{k_i^0} - \ket{k_i}}^2 =
2(1 - \Re \braket{k_i^0}{k_i}) \leq \frac{r^2}{n |c_i|^2} \; .
\eeq
We have assumed here that the contribution to the variance in $\ket{j^0_i}$ 
from the variance in $\ket{k^0_{i-1}}$ is negligible. This depends on the
coefficients $c_i$ vanishing sufficiently rapidly. In practice, while the
coefficients typically do get smaller quite quickly, this isn't something we can
really control as I don't know how to get a better bound than the condition number
of $H$, which typically will be much larger than $\frac{|c_{i-1}|}{|c_{i}|}$. In practice,
it will never be that bad. Or at least, we hope...

Note that this variance is approximately equal to the truncation error from the 
DMRG optimization of $\ket{k_i}$. In practice, it appears that the variance is
slightly smaller than the converged truncation error, not sure why, perhaps
it is simply a normalization issue?

Now, the triangle relation implies that the error after $N$ steps is bounded by
the sum of the errors for each step,
\beq
\norm{\ket{\psi^0(N \, \Delta t)} - \ket{\psi(N \, \Delta t)}}
\leq 
\sum_{i=1}^N
\norm{\ket{\psi^0(i \, \Delta t)} 
- U_i \, \ket{\psi^0((i-1)\Delta t)}} \; ,
\label{eq:sumerrors}
\eeq
where $U_i$ is the exact time evolution operator from time $(i-1)\Delta t$ to
$i \, \Delta t$, and we take the $t=0$ state to be exact, $\ket{\psi^0(0)} = \ket{\psi(0)}$.
If the error terms were \emph{independent}, we could instead write this in terms
of the variances $\norm{\ket{\psi(N \; \Delta t)} - \ket{\psi^0(N \; \Delta t)}}^2$.
Interesting question as to whether this is true or not: i.e., does a DMRG truncation have an
equivalent effect on the fidelity to adding a \emph{random, and independently distributed} 
term of appopriate magnitude? Probably not, as it is based on real-space truncation of the
entanglement which isn't random, but this needs to be checked. Results from Andreas for the
single-spin system show the numerical curve falling nicely in the middle of the strict error
bound and the `indpendent noise' curve. But it still needs checking.

So, basing our error term around \refeq{eq:sumerrors}, we fix an acceptable value for the
norm per unit timestep, $r_T$. Thus, for some timestep $\Delta t$, we require the variance
to be
\beq
r^2 = r_T^2 \, (\Delta t)^2 \; ,
\eeq
and the variance will increase as $t^2$.
Hence the bottom line variance of the Krylov vectors is
\beq
\norm{\ket{k_i^0} - \ket{k_i}}^2 \leq 
\frac{r_T^2 \, (\Delta t)^2}{n |c_i|^2} \; .
\eeq




\end{document}
